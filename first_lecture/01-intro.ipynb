{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248dbb37",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "**Fin 585**  \n",
    "**Diether**  \n",
    "**Python/Pandas Introduction**<br><br>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "+ Please read through my notes, and run each of the code cells.\n",
    "\n",
    "+ You can run a cell of code by pressing SHIFT and ENTER at the same time.\n",
    "\n",
    "\n",
    "**1 Python in Empirical Finance**\n",
    "\n",
    "**1.1 Role of Python in this Course**\n",
    "\n",
    "+ Goal: Develop the Python skills and tools necessary to engage in empirical research in Finance.\n",
    "\n",
    "+ More specifically, use Python/Pandas to $\\rightarrow$\n",
    "\n",
    "  - Test economic models.\n",
    "  \n",
    "  - Construct portfolios (container for financial assets).\n",
    "  \n",
    "  - Create and backtest trading strategies.\n",
    "  \n",
    "  - Estimate regressions: time series and panel regressions.\n",
    "  \n",
    "+ I will focus on the most important features and programming constructs in Python to accomplish these goals.\n",
    "\n",
    "+ Our main library will be `pandas`.<br><br>\n",
    "\n",
    "\n",
    "**1.2 Example: Portfolio Construction and Trading Strategies**\n",
    "\n",
    "+ A core quant finance and academic skill is portfolio construction and backtesting.\n",
    "\n",
    "+ All trading strategies are implemented as portfolios (container for financial assets).\n",
    "\n",
    "+ Portfolio construction and backtesting can be broken into five general steps:\n",
    "\n",
    "  1. Data preparation.\n",
    "\n",
    "  2. Creation of the portfolio formation variable.\n",
    "\n",
    "  3. Binning the stock return data based the formation variable.\n",
    "\n",
    "  4. Portfolio creation.\n",
    "\n",
    "  5. Estimating and benchmarking historical performance of the strategy.\n",
    "  \n",
    "+ Need to learn enough Python/Pandas so you can tackle each step for portfolio strategies you're interested in testing.<br><br>\n",
    "\n",
    "\n",
    "**1.3 Why Python?**\n",
    "\n",
    "+ Why Python? Why not R, SAS, Stata, or something else?\n",
    "\n",
    "+ All of those languages are used in empirical finance research.\n",
    "\n",
    "+ Python has some important advantages:\n",
    "\n",
    "  - Python has emerged as the most important of these languages in **Finance**.\n",
    "\n",
    "  - Well designed and popular general purpose programming language.<br><br>  \n",
    "\n",
    "\n",
    "**1.4 What about Polars Instead of Pandas?**\n",
    "\n",
    "+ `Polars` is really useful and worth learning.\n",
    "\n",
    "+ Particularly in Finance.\n",
    "\n",
    "+ It's multi-threading is better than `Pandas`.\n",
    "\n",
    "+ I still think it's beneficial to start with `Pandas`.\n",
    "\n",
    "+ It's still the most important data analysis library in Python.\n",
    "\n",
    "+ Learning to use `Pandas` efficiently will help you tackle programming tasks in Finance better.\n",
    "\n",
    "+ `Pandas` \"penalizes\" you more for being inefficient than `Polars`. That's good for you at this stage of tackling Finance related problems.<br><br>\n",
    "\n",
    "\n",
    "**2 Overview of Basic Concepts and Features**\n",
    "\n",
    "+ Main purpose of this notebook is to introduce the `Pandas` library.\n",
    "\n",
    "+ `Pandas` is the main library for this course.\n",
    "\n",
    "+ Will overview core concepts and features of `pandas` for quant and academic finance.\n",
    "\n",
    "+ Will cover the concepts and features with more detail as we move forward.<br><br>\n",
    "\n",
    "\n",
    "**2.1 Accessing the Pandas Library**\n",
    "\n",
    "+ To use the Pandas library we have to tell Python that we want access to it.\n",
    "\n",
    "+ You make `pandas` accessible by using the `import` command.\n",
    "\n",
    "+ When importing the pandas' library, you also associate the library with a namespace: **I use pd**\n",
    "\n",
    "  - `pd` namespace is not required, but it's a strong convention.\n",
    "\n",
    "  - Given the `pd` namespace $\\rightarrow$ Pandas' functions look like `pd.function`.\n",
    "  \n",
    "  - For example, `pd.read_csv`. \n",
    "\n",
    "  - Namespaces make it clear what library a certain function or command comes from if each library you use has it's own namespace.\n",
    "\n",
    "+ **code for importing pandas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5f758",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "<br>\n",
    "\n",
    "**2.2 Pandas core Data Structures: Dataframes and Series**\n",
    "\n",
    "+ Core data structure/object $\\rightarrow$ the `dataframe`.\n",
    "\n",
    "+ Dataframe: container for holding rectangular array of mixed type data.\n",
    "\n",
    "  - Columns: represent different variables (e.g, stock price or earnings of Google).\n",
    "\n",
    "  - Rows: represent a given observation for those variables (e.g., January 2009 for Google).\n",
    "\n",
    "+ It's the programming equivalent of a spreadsheet.\n",
    "\n",
    "+ Each column can be of a different type: integers, floating point numbers,  strings, or even imaginary numbers. <br><br>\n",
    "\n",
    "\n",
    "**2.3 Dataframes: Store Data and Provide Useful Functions**\n",
    "\n",
    "+ With `dataframes`, Pandas' provides:\n",
    "\n",
    "  - Ways to create new data.\n",
    "\n",
    "  - Transform and combine data.\n",
    "\n",
    "  - Aggregate data\n",
    "\n",
    "  - Display data. \n",
    "\n",
    "+ Examples:\n",
    "\n",
    "  - Built in division operator (`/`): divides element by element.\n",
    "\n",
    "  - Built in mean function that computes sample average of each column.\n",
    "  \n",
    "  - Higher level functions: e.g., plotting functions\n",
    "  \n",
    "+ Many functions are built into the `dataframe` object.\n",
    "\n",
    "+ Built in functions are called `methods` (object oriented programming terminology).<br><br>\n",
    "\n",
    "\n",
    "**2.4 Summary of DataFrames**\n",
    "\n",
    "+ A `dataframe` is an object that provides data storage and useful functions<br><br>\n",
    "\n",
    "\n",
    "**2.5 Series**\n",
    "\n",
    "+ A `series` in pandas' is the name for a single column of data.\n",
    "  \n",
    "+ If you grab one column of a dataframe, you're grabbing a series.\n",
    "\n",
    "+ `Dataframes` and `series` behave very similarly. For our purposes, it's mostly just be a technical distinction between a one dimensional and two dimensional array.<br><br>\n",
    "\n",
    "\n",
    "**2.6 Importing Data and Creating a DataFrame:**\n",
    "\n",
    "+ Getting data into a `Pandas'` dataframe is usually easy. \n",
    "\n",
    "+ Pandas can read many different data formats: csv files, Excel files, SAS data files, Stata data files (dta), Feather data files, etc.\n",
    "\n",
    "+ In this class, we mostly use csv files.\n",
    "\n",
    "+ I will highlight other methods.<br><br>\n",
    "\n",
    "\n",
    "**3. Reading in Some Financial Data**\n",
    "\n",
    "+ Let's read in some data, and create a `dataframe` object.\n",
    "\n",
    "+ Data: annual balance sheet data for Amazon and Hormel.\n",
    "\n",
    "+ Data are in a csv file $\\rightarrow$ use the `read_csv` function.\n",
    "\n",
    "+ The `read_csv` function will automatically create a `dataframe` object containing the data in the csv file.\n",
    "\n",
    "+ The `read_csv` function has a lot of options and flexibility.\n",
    "\n",
    "+ Often you don't use any of the options. Particularly for well formed csv files. \n",
    "\n",
    "+ [Pandas' documentation for read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n",
    "\n",
    "+ The `read_csv` function can, of course, read files stored on your local machine.\n",
    "\n",
    "+ It can also read files stored remotely on a webserver (just provide a URL). \n",
    "\n",
    "+ The code below calls pandas' `read_csv` function and then reads the csv located at the URL in quotes. After reading the file it creates a dataframe and assigns the dataframe to `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bff868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://diether.org/prephd/01-intro.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cbbc3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "+ To read in data from non-csv formats you generally invoke a command very similar to `read_csv`. For example, you can read in a `Stata` datafile using the following:\n",
    "```python\n",
    "    df = pd.read_stata('filename.dta')\n",
    "```\n",
    "\n",
    "+ Many other ways to create dataframes. Can convert core Python `lists` or `dictionaries` into `dataframes`.<br><br> \n",
    "\n",
    "\n",
    "**3.1 Displaying or Printing out the Data in a Dataframe**\n",
    "\n",
    "+ A **Jupyter notebook** is a special environment where if you type the name of a dataframe (or other datatypes), it will display the default view of that object.\n",
    "\n",
    "+ If the dataframe is small it will display all the data.\n",
    "\n",
    "+ if the dataframe is large only a truncated view of the data will be displayed. \n",
    "\n",
    "+ If you write a python program and run it outside of the Jupyter notebook environment, then you need to use the `print` function to see any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab337da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7854d61",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<bf>\n",
    "\n",
    "**Print function**\n",
    "\n",
    "+ You can also explicitly print a dataframe out using python's print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2390b12",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<br>\n",
    "\n",
    "**3.2 Dataframes and Series**\n",
    "\n",
    "+ Our `dataframe` is called `df`.\n",
    "\n",
    "+ If we select a column from the `dataframe` it will be of type `series`.\n",
    "\n",
    "+ We select a column of a dataframe (a series) by wrapping the column's name in quotes.\n",
    "\n",
    "  - You typically must wrap the column's name in quotes because most column names are stored as strings.\n",
    "\n",
    "  - You will need to reference columns this way as long as the variable names aren't entirely numeric. \n",
    "\n",
    "  - Can delimit strings with single or double quotes: ' ' or \" \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df850689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tick','year','revenue']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17320c6d",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "<br>\n",
    "\n",
    "**3.3 Checking the Data Type**\n",
    "\n",
    "+ In Python, there is a `type` function that returns the type of a variable or object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['revenue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4987ad",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "<br>\n",
    "\n",
    "**3.4 Data creation** \n",
    "\n",
    "+ Typically create a new column with the assignment operator.\n",
    "\n",
    "+ Like most programming languages, the assignment operator is just the equal sign (`=`) in Python.\n",
    "\n",
    "+ For example, suppose I want to create a new column that measures profit margin. Profit margin is defined as the following (note, ebit is earnings before interest and taxes):\n",
    "$$\n",
    "\\text{Profit Margin} = \\frac{ebit}{revenue}\n",
    "$$\n",
    "\n",
    "+ Mathematical operations such as addition (+), subtraction (-), multiplication (*), or division (/) are all element by element operations between the dataframe columns that are addressed by the code.\n",
    "\n",
    "+ Python/Pandas code for creating profit margin column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['profit_margin'] = df['ebit'] / df['revenue']\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ff3d3",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "<br>\n",
    "\n",
    "**3.5 If/then/else logic in Pandas**\n",
    "\n",
    "+ `If/then/else` logic is important in all types of programming.\n",
    "\n",
    "+ In `Python/Pandas`, you rarely write code that looks like classic `if/then/else` statements.\n",
    "\n",
    "+ For example, many `Pandas` logical functions or statements are actually `if/then` statements with an implicit else.\n",
    "\n",
    "+ Data selection often involves if/then/else logic $\\leftarrow$ in Pandas' jargon it's often called Boolean indexing.\n",
    "\n",
    "+ For example, we can use if/then/else logic to create a new variable that is `True` if the year is greater than 2010 and `False` otherwise. The logical statement looks like the following:\n",
    "\n",
    "```\n",
    "if (year is greater than 2010) then\n",
    "   True\n",
    "else\n",
    "   False\n",
    "```\n",
    "\n",
    "+ In Pandas, we create a logical condition for the whole column $\\rightarrow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] > 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818a826",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "+ We can also assign this new TRUE/FALSE variable to the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gt_2010'] = df['year'] > 2010\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76609b75",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "<br>\n",
    "\n",
    "**3.6 Data selection** \n",
    "\n",
    "+ Based on if/then/else logic, `Pandas` allows you to select only the rows or columns of a `dataframe` that you want.\n",
    "\n",
    "+ Suppose you only want observations where the year is greater than 2010.\n",
    "\n",
    "+ Pandas allow us to index a dataframe's rows based on a logical condition.\n",
    "\n",
    "+ Pandas will select on observations where the condition is `TRUE`.\n",
    "\n",
    "+ Called logical indexing $\\rightarrow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145644b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['gt_2010'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dccc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['gt_2010']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['year'] > 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c8a29",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<br>\n",
    "\n",
    "**3.7 Creating a Sub Dataframe**\n",
    "\n",
    "+ We can assign the smaller dataframe to a new dataframe with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0199be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = df[df['year'] > 2010]\n",
    "sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605aeed2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<br>\n",
    "\n",
    "**3.8 Deleting a Variable/Column of Data**\n",
    "\n",
    "+ Very common to delete or remove columns.\n",
    "\n",
    "+ Typically rely on the `drop` function.\n",
    "\n",
    "+ For example, suppose I want to drop the `capx` column from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('capx',axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d6747",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "+ The preceding command, created a new `dataframe` with the `capx` column removed. \n",
    "\n",
    "+ Most `pandas` functions create a new dataframe.\n",
    "\n",
    "+ To modify the original `dataframe` (df) we have to assign the `dataframe` created by the drop command back to `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a405a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('capx',axis='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e01f8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<br>\n",
    "\n",
    "**4 More Advanced Concepts and Features for Later**\n",
    "\n",
    "**4.1 The groupby/apply construct:**. \n",
    "\n",
    "+ The most important **programming idiom** or construct for this class is the `groupby/apply` construct.\n",
    "\n",
    "+ Allows us to loop through the data grouping observations in a `dataframe` together, and then apply a function to each group.\n",
    "\n",
    "+ For example, we often use it to group observations by date or to group all the observation of the same stock together. We then typically apply a function to the data that aggregates it or transforms it within these groups.\n",
    "\n",
    "+ The `groupby/apply` construct allows us to accomplish the following with just one (or a few lines) of code:\n",
    "\n",
    "  1. Logically **group** observations together based on some attribute of the data: for example, we could group stock data based on whether the company was big or small.\n",
    "\n",
    "  2. **Apply** a function to the different groups. For example, we could compute the average number of analysts covering big versus small stocks.\n",
    "\n",
    "+ The groupby/apply does a whole bunch of work for us behind the scene. It loops all the observations, categorizes the observations into the groups, and then applies the functions separately to each group.<br><br>\n",
    "\n",
    "\n",
    "**4.2 User-written functions:**\n",
    "\n",
    "+ You will write your own custom (i.e., user written) functions to extend the functionality of the `groupby/apply` construct.\n",
    "\n",
    "+ For example, writing a custom function is sometimes and important part of implementing a portfolio formation criteria for a trading strategy.<br><br>\n",
    "\n",
    "\n",
    "**4.3 Merging data** \n",
    "\n",
    "+ Merging data is a core part of the data preparation step from most empirical work or back testing of strategies.\n",
    "\n",
    "+ You will learn how to merge dataframes together based on a single key or multiple keys."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}